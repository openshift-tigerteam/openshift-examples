{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"OpenShift Examples","text":""},{"location":"#install","title":"Install","text":"<p>Prework, Example install configs, etc. </p>"},{"location":"#post-install","title":"Post-Install","text":"<p>Post cluster installation configurations and other work </p>"},{"location":"#networking","title":"Networking","text":"<p>Examples of <code>NodeNetworkConfigurationPolicy</code> and <code>ClusterUserDefinedNetwork</code></p>"},{"location":"#virtualization","title":"Virtualization","text":"<p>Examples for OpenShift Virtualization</p>"},{"location":"#storage","title":"Storage","text":"<p>Examples for CSI providers for OpenShift</p>"},{"location":"#hub-cluster","title":"Hub Cluster","text":"<p>Building a Hub cluster with SNO + ACM</p>"},{"location":"#notes","title":"Notes","text":"<p>Assorted notes for OpenShift install and configuration</p> <p>Base64 and MachineConfig </p>"},{"location":"hub/hub/","title":"Creating the Hub Cluster","text":""},{"location":"hub/hub/#hardware-requirements","title":"Hardware Requirements","text":"<ul> <li>Single box. Recommend 24 cores, 64 GB RAM and two disks - os disk 120 GB, data disk 2TB</li> <li>Can be bare metal or virtualized. No OpenShift Virtualization here. </li> </ul>"},{"location":"hub/hub/#install-sno","title":"Install SNO","text":"<ul> <li>Highly recommend assisted installer for this. </li> </ul>"},{"location":"hub/hub/#install-storage","title":"Install Storage","text":"<p>These examples are in leiu of existing storage. </p>"},{"location":"hub/hub/#install-storage-operators","title":"Install Storage Operators","text":"<ul> <li>LVM Storage Operator</li> <li>Openshift Data Foundation (ODF) Operator  </li> </ul> <p>Label the node as a storage node  </p> <pre><code>oc label node &lt;node-name&gt; cluster.ocs.openshift.io/openshift-storage=\n</code></pre>"},{"location":"hub/hub/#create-lvm-storage","title":"Create LVM Storage","text":"<p>Below is an example. Change out the path for your data disk. </p> <pre><code>cat &lt;&lt;EOF | oc apply -f -\napiVersion: lvm.topolvm.io/v1alpha1\nkind: LVMCluster\nmetadata:\n  name: local-storage-lvm-cluster\n  namespace: openshift-storage\nspec:\n  storage:\n    deviceClasses:\n      - name: local-storage\n        default: true\n        fstype: xfs\n        deviceSelector:\n          paths:\n            - /dev/nvme0n1\n        thinPoolConfig:\n          name: thin-pool-1\n          sizePercent: 90\n          overprovisionRatio: 10\n          chunkSizeCalculationPolicy: Static\n          metadataSizeCalculationPolicy: Host\nEOF\n</code></pre>"},{"location":"hub/hub/#create-odf-storagecluster-for-objectstorage","title":"Create ODF StorageCluster for ObjectStorage","text":"<p>We are using ODF only for the ObjectStorage</p> <pre><code>cat &lt;&lt;EOF | oc apply -f -\napiVersion: ocs.openshift.io/v1\nkind: StorageCluster\nmetadata:\n  name: mcog-storagecluster\n  namespace: openshift-storage\nspec:\n  arbiter: {}\n  encryption:\n    keyRotation:\n      schedule: '@weekly'\n    kms: {}\n  externalStorage: {}\n  managedResources:\n    cephObjectStoreUsers: {}\n    cephCluster: {}\n    cephBlockPools: {}\n    cephNonResilientPools: {}\n    cephObjectStores: {}\n    cephFilesystems: {}\n    cephRBDMirror: {}\n    cephToolbox: {}\n    cephDashboard: {}\n    cephConfig: {}\n  multiCloudGateway:\n    dbStorageClassName: lvms-local-storage\n    reconcileStrategy: standalone\n  resourceProfile: balanced\nEOF\n</code></pre>"},{"location":"hub/hub/#install-multiclusterobservability","title":"Install MultiClusterObservability","text":"<ul> <li>Storage is installed (above)</li> </ul> <pre><code>oc create namespace open-cluster-management-observability\nDOCKER_CONFIG_JSON=`oc extract secret/pull-secret -n openshift-config --to=-`\noc create secret generic multiclusterhub-operator-pull-secret \\\n    -n open-cluster-management-observability \\\n    --from-literal=.dockerconfigjson=\"$DOCKER_CONFIG_JSON\" \\\n    --type=kubernetes.io/dockerconfigjson\n</code></pre> <pre><code>cat &lt;&lt;EOF | oc apply -f -\napiVersion: objectbucket.io/v1alpha1\nkind: ObjectBucketClaim\nmetadata:\n  name: thanos-object-storage-obc\n  namespace: open-cluster-management-observability\nspec:\n  bucketName: thanos-object-storage-bucket\n  storageClassName: openshift-storage.noobaa.io\nEOF\n</code></pre> <pre><code>ACCESS_KEY=$(oc get secret thanos-object-storage-obc -n open-cluster-management-observability -o jsonpath='{.data.AWS_ACCESS_KEY_ID}' | base64 -d)\nSECRET_KEY=$(oc get secret thanos-object-storage-obc -n open-cluster-management-observability -o jsonpath='{.data.AWS_SECRET_ACCESS_KEY}' | base64 -d)\n</code></pre> <pre><code>cat &lt;&lt;EOF | oc apply -f -\napiVersion: v1\nkind: Secret\nmetadata:\n  name: thanos-object-storage\n  namespace: open-cluster-management-observability\ntype: Opaque\nstringData:\n  thanos.yaml: |\n    type: s3\n    config:\n      bucket: thanos-object-storage-bucket\n      endpoint: s3.openshift-storage.svc:443\n      insecure: false\n      access_key: $ACCESS_KEY\n      secret_key: $SECRET_KEY\nEOF\n</code></pre> <pre><code>cat &lt;&lt;EOF | oc apply -f -\napiVersion: observability.open-cluster-management.io/v1beta2\nkind: MultiClusterObservability\nmetadata:\n  name: multi-cluster-observability\nspec:\n  enableDownsampling: true\n  imagePullPolicy: Always\n  imagePullSecret: multiclusterhub-operator-pull-secret\n  observabilityAddonSpec:\n    enableMetrics: true\n    interval: 300\n  storageConfig:\n    alertmanagerStorageSize: 1Gi\n    compactStorageSize: 100Gi\n    metricObjectStorage:\n      key: thanos.yaml\n      name: thanos-object-storage\n    receiveStorageSize: 100Gi\n    ruleStorageSize: 1Gi\n    storageClass: lvms-local-storage\n    storeStorageSize: 10Gi\nEOF\n</code></pre>"},{"location":"hub/hub/#install-acm","title":"Install ACM","text":"<ul> <li>Install Advanced Cluster Management Operator</li> </ul> <p>Add Provisioning</p> <pre><code>apiVersion: metal3.io/v1alpha1\nkind: Provisioning\nmetadata:\n  name: provisioning-configuration\nspec:\n  provisioningNetwork: \"Disabled\"\n  watchAllNamespaces: true\n</code></pre>"},{"location":"hub/hub/#create-infraenv","title":"Create InfraEnv","text":"<pre><code>cat &lt;&lt;EOF | oc apply -f -\napiVersion: agent-install.openshift.io/v1beta1\nkind: InfraEnv\nmetadata:\n  name: lab\n  namespace: lab\nspec:\n  agentLabels:\n    agentclusterinstalls.extensions.hive.openshift.io/location: ktown\n  cpuArchitecture: x86_64\n  ipxeScriptType: DiscoveryImageAlways\n  nmStateConfigLabelSelector:\n    matchLabels:\n      infraenvs.agent-install.openshift.io: lab\n  pullSecretRef:\n    name: pullsecret-lab\n  sshAuthorizedKey: &lt;public-key&gt;\nEOF\n</code></pre>"},{"location":"hub/hub/#adding-host-inventory-via-redfish","title":"Adding Host Inventory via Redfish","text":"<ul> <li>Advanced Cluster Management for Kubernetes is installed</li> <li>Bare Metal Operator is installed</li> <li>Redfish credentials are available for the target host</li> </ul> <pre><code>oc create secret generic &lt;hostname&gt;-bmc-secret \\\n  --from-literal=username=admin \\\n  --from-literal=password=your-bmc-password \\\n  -n &lt;InfraEnv-namespace&gt;\n</code></pre> <pre><code>echo -n \"admin\" | base64\necho -n \"your-bmc-password\" | base64  \n\ncat &lt;&lt;EOF | oc apply -f -\napiVersion: v1\nkind: Secret\nmetadata:\n  name: &lt;hostname&gt;-bmc-secret\n  namespace: &lt;InfraEnv-namespace&gt;\ntype: Opaque\ndata:\n  username: &lt;base64-encoded-username&gt;\n  password: &lt;base64-encoded-password&gt;\nEOF\n</code></pre> <pre><code>apiVersion: metal3.io/v1alpha1\nkind: BareMetalHost\nmetadata:\n  name: &lt;hostname&gt;-bmh\n  namespace: &lt;InfraEnv-namespace&gt;\n  labels:\n    infraenvs.agent-install.openshift.io: InfraEnv-namespace\nspec:\n  bmc:\n    address: redfish-virtualmedia://&lt;bmc-ip&gt;/redfish/v1/\n    credentialsName: \"&lt;hostname&gt;-bmc-secret\"\n    disableCertificateVerification: true\n  bootMACAddress: \"aa:bb:cc:dd:ee:ff\"\n  online: false\n</code></pre>"},{"location":"install/baremetal_hardware/","title":"Baremetal hardware","text":""},{"location":"install/baremetal_hardware/#required-hardware","title":"Required Hardware","text":"<p>Below is a list of the recommended hardware for a basic OpenShift install. Consider these minimum values - the more the better. Disks must be SSD/NVME. </p> Hosts (7 total) CPU Memory Install Disk bastion 4 16 GB 60 GB openshift-control-plane-{1,2,3} 16 32 GB 120 GB openshift-worker-{1,2,3} 16 64 GB 120 GB <p>This also assumes your storage is handled elsewhere is already in place. If ODF will be used for storage, then worker machines would need additional CPU, RAM and an extra SSD/NVME drive in at least three of the worker machines to form a storage cluster. </p> <p>There is an ability to run the control plane nodes as schedulable, making them effectively worker nodes as well. This is discouraged unless required by the environment constraints, such as testing to run edge systems. </p> <p>Run etcd on a block device that can write at least 50 IOPS of 8KB sequentially, including fdatasync, in under 10ms. Consider moving etcd to it's own disk</p>"},{"location":"install/bastion/","title":"Bastion","text":"<p>The bastion host serves multiple purposes. First, all of our install work is done here. Second, it's used to validate the environment prior to installation. Third, it provides additional tools for installation such as a web server environment for the ISO images. </p>"},{"location":"install/bastion/#install-the-os","title":"Install the OS","text":"<ul> <li>Download Red Hat Enterprise Linux 9.x Binary DVD</li> <li>Boot host from ISO and perform install as <code>Server with GUI</code></li> <li>Make sure to enable SSH for user</li> <li>Reboot and SSH into bastion host as administrative user</li> </ul>"},{"location":"install/bastion/#register-the-host","title":"Register the Host","text":"<p>To be able to install necessary packages, you need to register the host with your Red Hat ID and password. You could have also accomplished this during the RHEL install. </p> <pre><code>sudo subscription-manager register # Enter username/password\nsudo subscription-manager repos --enable=rhel-9-for-x86_64-baseos-rpms\nsudo subscription-manager repos --enable=rhel-9-for-x86_64-appstream-rpms\nsudo dnf update -y \nsudo reboot\n</code></pre>"},{"location":"install/bastion/#download-required-tools","title":"Download Required Tools","text":"<p>Install the required tools.   </p> <pre><code>OCP_VERSION=4.19\nwget \"https://mirror.openshift.com/pub/openshift-v4/x86_64/clients/ocp/stable-${OCP_VERSION}/openshift-install-linux.tar.gz\" -P /tmp\nsudo tar -xvzf /tmp/openshift-install-linux.tar.gz -C /usr/local/bin\nwget \"https://mirror.openshift.com/pub/openshift-v4/clients/ocp/stable-${OCP_VERSION}/openshift-client-linux.tar.gz\" -P /tmp\nsudo tar -xvzf /tmp/openshift-client-linux.tar.gz -C /usr/local/bin\nrm /tmp/openshift-install-linux.tar.gz /tmp/openshift-client-linux.tar.gz -y\nsudo dnf install nmstate git podman \n</code></pre> <p>Check for the availability of the required tools.    </p> <pre><code>openshift-install version\noc version\nnmstatectl -V\ngit -v\npodman --version\n</code></pre>"},{"location":"install/bastion/#get-the-pull-secret","title":"Get the Pull Secret","text":"<p>Pull Secret is available at https://console.redhat.com/openshift/install/pull-secret Download to ~/.pull-secret</p>"},{"location":"install/bastion/#create-ssh-key","title":"Create SSH Key","text":"<pre><code>ssh-keygen -t ed25519 -f ~/.ssh/ocp_ed25519\n</code></pre>"},{"location":"install/bastion/#open-port-8080-for-iso-http","title":"Open Port 8080 for iso-http","text":"<pre><code>sudo firewall-cmd --permanent --add-port=8080/tcp\nsudo firewall-cmd --reload\n</code></pre>"},{"location":"install/bastion/#tools-for-environment-validation","title":"Tools for Environment Validation","text":"<pre><code>ping registry.redhat.io        # ICMP doesn\u2019t always work, but try\ncurl -vk https://registry.redhat.io/v2/\ndig registry.redhat.io +short\nnslookup registry.redhat.io\npodman login registry.redhat.io\n</code></pre>"},{"location":"install/config/","title":"Config","text":"<p>The typical agent based install requires two configuration files. </p> <ul> <li><code>install-config.yaml</code></li> <li><code>agent-config.yaml</code></li> </ul> <p>The <code>install-config.yaml</code> contains all the cluster level information while the <code>agent-config.yaml</code> provides host level configurations. </p> <p>You can accomplish all of this with bastion tools like <code>vi</code>. It is highly recommended to use tools like Visual Studio Code (vscode) to help with yaml formatting. </p>"},{"location":"install/config/#create-the-working-directory","title":"Create the Working Directory","text":"<p>You are going to want to create a working directory and initialize it into a git repository. </p> <pre><code>mkdir -p ocp &amp;&amp; cd ocp\ngit init \necho \"install/\" &gt; .gitignore\necho \"# POC install notes\" &gt; notes.md\ntouch install-config.yaml\ntouch agent-config.yaml\ngit add -A\ngit commit -m \"repo initialized\"\n</code></pre>"},{"location":"install/config/#create-the-install-config","title":"Create the Install Config","text":"<p>Here is an example of the <code>install-config.yaml</code> contents.</p> <pre><code>apiVersion: v1\nbaseDomain: ocp.basedomain.com\nmetadata:\n  name: poc\ncontrolPlane:\n  name: master\n  architecture: amd64\n  hyperthreading: Enabled\n  replicas: 3\ncompute:\n- name: worker\n  architecture: amd64\n  hyperthreading: Enabled\n  replicas: 3\nnetworking:\n  clusterNetwork:\n  - cidr: 10.128.0.0/14\n    hostPrefix: 23\n  machineNetwork:\n  - cidr: 10.1.0.0/24\n  networkType: OVNKubernetes\n  serviceNetwork:\n  - 172.30.0.0/16\nplatform:\n  baremetal:\n    apiVIP: 10.1.0.9\n    ingressVIP: 10.1.0.10\n    additionalNTPServers: \n      - 0.us.pool.ntp.org\n      - 1.us.pool.ntp.org\npullSecret: 'value from ~/.pull_secret'\nsshKey: 'value from ~/.ssh/ocp_ed25519.pub'\n</code></pre> <p>If your environment requires a proxy to access the internet then copy, modify and append to the end of the <code>install-config.yaml</code></p> <pre><code>proxy:\n  httpProxy: http://user:password@proxy.example.com:3128\n  httpsProxy: http://user:password@proxy.example.com:3128\n  noProxy: poc.ocp.basedomain.com,10.1.0.0/24,10.128.0.0/14,172.30.0.0/16,api.poc.ocp.basedomain.com,api-int.poc.ocp.basedomain.com,*.apps.poc.ocp.basedomain.com\n</code></pre>"},{"location":"install/config/#create-the-agent-config","title":"Create the Agent Config","text":"<p>The agent-config.yaml is basically a list of hosts' information. You'll create a list item for each of the hosts. Here's an example of a host with two ethernet connections bonded together in an LACP bond. </p> <pre><code>apiVersion: v1alpha1\nkind: AgentConfig\nmetadata:\n  name: poc\nrendezvousIP: 10.1.0.11                     # IP of the first control-plane1 host\nadditionalNtpSources:\n  - 0.us.pool.ntp.org\n  - 1.us.pool.ntp.org\nhosts:\n  - hostname: openshift-control-plane-1     # hostname\n    role: master                            # master/worker\n    rootDeviceHints:\n      deviceName: \"/dev/sda\"                # Disk Hint\n    interfaces:\n      - name: eno1                          # Interface Name 1\n        macAddress: A1:B2:3C:4D:1E:11       # Mac Address 1\n      - name: eno2                          # Interface Name 2\n        macAddress: A1:B2:3C:4D:2E:11       # Mac Address 2\n    networkConfig:\n      interfaces:\n        - name: eno1                        # Interface Name 1\n          type: ethernet\n          state: up\n          mac-address: A1:B2:3C:4D:1E:11    # Mac Address 1\n          ipv4:\n            enabled: false\n          ipv6:\n            enabled: false\n        - name: eno2                        # Interface Name 2\n          type: ethernet\n          state: up\n          mac-address: A1:B2:3C:4D:2E:11    # Mac Address 2\n          ipv4:\n            enabled: false\n          ipv6:\n            enabled: false\n        - name: bond0\n          type: bond\n          state: up\n          ipv4:\n            enabled: true\n            address:\n              - ip: 10.1.0.11               # Host IP inside the machine subnet\n                prefix-length: 24\n            dhcp: false\n          link-aggregation:\n            mode: 802.3ad                   # LACP\n            port:\n              - eno1\n              - eno2\n            options:\n              miimon: \"100\"\n              lacp_rate: fast\n          ipv6:\n            enabled: false\n      dns-resolver:\n        config:\n          server:\n            - dns1.basedomain.com           # DNS 1\n            - dns2.basedomain.com           # DNS 2\n      routes:\n        config:\n          - destination: 0.0.0.0/0\n            next-hop-address: 10.1.0.1\n            next-hop-interface: bond0\n            table-id: 254\n</code></pre> <p>Notice the inconsistent labels and spellings.   * <code>macAddress</code> in the interfaces stanza, but <code>mac-address</code> in the networkConfig stanza.     * <code>additionalNtpSources</code> is used here, but it's <code>additionalNTPServers</code> in the <code>install-config.yaml</code></p> <p>Docs: Sample Config Bonds Vlans</p>"},{"location":"install/httpd/","title":"Httpd","text":"<pre><code>\nsudo dnf install -y nginx\n\ncat &lt;&lt;EOF | sudo tee /etc/nginx/conf.d/iso.conf\nserver {\n    listen       8080;\n    server_name  _;\n\n    location / {\n        root   /home/user1/ocp/install;\n        autoindex on;\n    }\n}\nEOF\n\nsudo chcon -Rt httpd_sys_content_t /home/user1/ocp/install\n\nsudo firewall-cmd --permanent --add-port=8080/tcp\nsudo firewall-cmd --reload\n\nsudo systemctl enable --now nginx\n</code></pre> <p>Test it</p> <pre><code>curl http://localhost/agent.x86_64.iso --head\n</code></pre>"},{"location":"install/install/","title":"Install","text":"<p>These install documents are focused on on-premise installations on bare metal. There are other install methods. If you can use the Assisted Installer, do so. </p> <p>You will need an entitled Red Hat account. </p> <p>Please read the entire page prior to starting. It is also recommended to acquire a git repository. </p>"},{"location":"install/install/#acquire-the-hardware","title":"Acquire the Hardware","text":""},{"location":"install/install/#required-hardware","title":"Required Hardware","text":"<p>Below is a list of the recommended hardware for a basic OpenShift install. Consider these minimum values - the more the better. Disks must be SSD/NVME. </p> Hosts (7 total) CPU Memory Install Disk bastion 4 16 GB 60 GB openshift-control-plane-{1,2,3} 16 32 GB 120 GB openshift-worker-{1,2,3} 16 64 GB 120 GB <p>This also assumes your storage is handled elsewhere is already in place. If ODF will be used for storage, then worker machines would need additional CPU, RAM and an extra SSD/NVME drive in at least three of the worker machines to form a storage cluster. </p> <p>There is an ability to run the control plane nodes as schedulable, making them effectively worker nodes as well. This is discouraged unless required by the environment constraints, such as testing to run edge systems. </p> <p>Run etcd on a block device that can write at least 50 IOPS of 8KB sequentially, including fdatasync, in under 10ms. Consider moving etcd to it's own disk</p>"},{"location":"install/install/#gather-the-information","title":"Gather the Information","text":""},{"location":"install/install/#cluster-values","title":"Cluster Values","text":"<p>Below are the values an enterprise typically has to gather or create for installing OpenShift.</p> Example Value Description Cluster Name poc Name of the cluster Base Domain ocp.basedomain.com Name of the domain Cluster Suffix poc.ocp.basedomain.com cluster name + base domain, for easier notation Machine Subnet 10.1.0.0/24 (vlan - 123) Subnet and vlan for all machines/VIPs in cluster Pod Subnet 10.128.0.0/14 Subnet for pod SDN Pod Subnet - Host Prefix 23 Host prefix for Subnet for pod SDN Service Subnet 172.30.0.0/16 Subnet for service SDN API VIP 10.1.0.9 VIP for the MetalLB API Endpoint * Ingress VIP 10.1.0.10 VIP for the MetalLB Ingress Endpoint * DNS dns1.basedomain.com,etc IP or hostname for the DNS hosts NTP ntp.basedomain.com,etc IP or hostname for the NTP hosts <ul> <li>VIPs must be within the machine subnet</li> </ul>"},{"location":"install/install/#sdn-subnet-overlaps","title":"SDN Subnet Overlaps","text":"<p>The Pod Subnet and Service Subnet are run in the software defined network (SDN) of the cluster. If those values conflict with existing subnets and your pods in the cluster will want to route to those outside services with conflicting IPs, you will need to provide different subnets. It's much easier to ensure they are unique.  </p>"},{"location":"install/install/#pod-subnet-and-host-prefix-explanation","title":"Pod Subnet and Host Prefix Explanation","text":"<p>If we are just doing a small cluster, think 500 pods per node and 6 nodes is 3000 ips. You could use a <code>/20</code> and then each node would have a <code>/23</code>. <code>10.128.0.0/20</code> with <code>hostPrefix: 23</code>. </p> <pre><code>10.128.0.0/23    (10.128.0.0 \u2013 10.128.1.255)\n10.128.2.0/23    (10.128.2.0 \u2013 10.128.3.255)\n10.128.4.0/23    (10.128.4.0 \u2013 10.128.5.255)\n10.128.6.0/23    (10.128.6.0 \u2013 10.128.7.255)\n10.128.8.0/23    (10.128.8.0 \u2013 10.128.9.255)\n10.128.10.0/23   (10.128.10.0 \u2013 10.128.11.255)\n10.128.12.0/23   (10.128.12.0 \u2013 10.128.13.255)\n10.128.14.0/23   (10.128.14.0 \u2013 10.128.15.255)\n</code></pre> <p>Pod Subnet - Host Prefix CIDR Subnet Reference</p>"},{"location":"install/install/#machine-information-required","title":"Machine Information Required","text":"<p>Typically, machines will have more than one NIC and these will be setup in a bond. Please collect the interface names and MAC addresses for ALL NICS and the install disk location on the machines. You provide the hostnames, IPs. IPs need to be located in the machine configuration subnet used on the install. Below is an example table of values needed for collection. </p> Hostname Interface MAC Address Bond IP Disk Hint bastion eno1 A0-B1-C2-D3-E4-E1 10.1.0.5 /dev/sda openshift-control-plane-1 eno1 A0-B1-C2-D3-E4-E1 10.1.0.11 /dev/sda eno2 A0-B1-C2-D3-E4-E2 openshift-control-plane-2 eno1 A0-B1-C2-D3-E4-E3 10.1.0.12 /dev/sda eno2 A0-B1-C2-D3-E4-E4 openshift-control-plane-3 eno1 A0-B1-C2-D3-E4-E5 10.1.0.13 /dev/sda eno2 A0-B1-C2-D3-E4-E6 openshift-worker-1 eno1 A0-B1-C2-D3-E4-F1 10.1.0.21 /dev/sda eno2 A0-B1-C2-D3-E4-F2 openshift-worker-2 eno1 A0-B1-C2-D3-E4-F3 10.1.0.22 /dev/sda eno2 A0-B1-C2-D3-E4-F4 openshift-worker-3 eno1 A0-B1-C2-D3-E4-F5 10.1.0.23 /dev/sda eno2 A0-B1-C2-D3-E4-F6 <p>The IPs in the example table represent a bonded IP or if the cluster does not use bonding, that IP that connects the machine to the machine network. Notice all machine IPs are inside of the machine subnet of 10.1.0.0/24 defined above. </p> <p>On modern RHEL (RHEL CoreOS included), the names of your NICs aren\u2019t the old eth0, eth1 style anymore. They use predictable network interface names, which are generated at boot based on hardware topology and firmware information. Here\u2019s the gist of how RHEL decides what your NICs will be called:</p> <ul> <li>eno1, eno2 \u2192 onboard NICs (from BIOS/firmware)</li> <li>ens1f0, ens1f1 \u2192 PCI Express slots (\u201cs\u201d = slot, \u201cf\u201d = function)</li> <li>enp3s0 \u2192 PCI bus location (p3 = bus 3, s0 = slot 0)</li> <li>enx \u2192 if nothing else matches, fall back to the MAC address <p>So the name is tied to where the NIC is physically, not just \u201cfirst one detected.\u201d If you don't know what they will be, boot one of the boxes with a RHEL ISO and find out.  </p> <p>Note: If you are using a hostname scheme that uses an integer at the end of the name, you should start with 1, not zero. But if you start with zero and you want it to match up with your ending IP, be consistent. </p>"},{"location":"install/install/#update-the-network","title":"Update the Network","text":""},{"location":"install/install/#firewall-and-networking-requirements","title":"Firewall and Networking Requirements","text":"<p>Prior to the install, you must open the firewall to connect to Red Hat's servers and ports between the machines.</p> <ul> <li>Configuring Firewall (note below)</li> <li>Network Connectivity Requirements</li> <li>Ensuring required ports are open</li> </ul> <p>Note: If you are using third-party software from a particular external repository, you will need to provide access to that as well. It is recommended to provide access to docker.io. </p>"},{"location":"install/install/#mitm-proxy-and-self-signed-certificates","title":"MITM Proxy and Self-Signed Certificates","text":"<p>If you are using a MITM proxy which is doing TLS inspection, the Red Hat CDN for <code>dnf install</code> or <code>yum install</code> uses cdn.redhat.com which has a self-signed certificate. If your MITM proxy automatically blocks self-signed certificates, you will need to whitelist cdn.redhat.com.</p> <p>It is recommended that all entries for the firewall as detailed above are also allowed through any TLS inspection processes. </p>"},{"location":"install/install/#create-dns-entries","title":"Create DNS Entries","text":"<p>Create the following A records in your DNS bases on the values from above. </p> A Record IP Address Description <code>api.&lt;cluster_suffix&gt;</code> 10.1.0.9 Virtual IP  (VIP) for the API endpoint <code>api-int.&lt;cluster_suffix&gt;</code> 10.1.0.9 Internal VIP for the API endpoint <code>*.apps.&lt;cluster_suffix&gt;</code> 10.1.0.10 Virtual IP for the ingress endpoint <p>You can validate the DNS using dig</p> <pre><code>dig +noall +answer @&lt;dns&gt; api.&lt;cluster_suffix&gt;\ndig +noall +answer @&lt;dns&gt; test.apps.&lt;cluster_suffix&gt;\n</code></pre> <p>DNS requirements Validating DNS resolution for user-provisioned infrastructure</p>"},{"location":"install/install/#optional-helpful-dns-entries","title":"Optional Helpful DNS Entries","text":"A Record IP Address Description <code>bastion.&lt;cluster_suffix&gt;</code> 10.1.0.4 IP for the bastion <code>openshift-control-plane-1.&lt;cluster_suffix&gt;</code> 10.1.0.11 IP for cp1 <code>openshift-control-plane-2.&lt;cluster_suffix&gt;</code> 10.1.0.12 IP for cp2 <code>openshift-control-plane-3.&lt;cluster_suffix&gt;</code> 10.1.0.13 IP for cp3 <code>openshift-worker-1.&lt;cluster_suffix&gt;</code> 10.1.0.21 IP for w1 <code>openshift-worker-2.&lt;cluster_suffix&gt;</code> 10.1.0.22 IP for w2 <code>openshift-worker-3.&lt;cluster_suffix&gt;</code> 10.1.0.23 IP for w3"},{"location":"install/install/#build-the-bastion-host","title":"Build the Bastion Host","text":"<p>The bastion host serves multiple purposes. First, all of our install work is done here. Second, it's used to validate the environment prior to installation. Third, it provides additional tools for installation such as a web server environment for the ISO images. </p>"},{"location":"install/install/#install-the-os","title":"Install the OS","text":"<ul> <li>Download Red Hat Enterprise Linux 9.x Binary DVD</li> <li>Boot host from ISO and perform install as <code>Server with GUI</code></li> <li>Make sure to enable SSH for user</li> <li>Reboot and SSH into bastion host as administrative user</li> </ul>"},{"location":"install/install/#register-the-host","title":"Register the Host","text":"<p>To be able to install necessary packages, you need to register the host with your Red Hat ID and password. You could have also accomplished this during the RHEL install. </p> <pre><code>sudo subscription-manager register # Enter username/password\nsudo subscription-manager repos --enable=rhel-9-for-x86_64-baseos-rpms\nsudo subscription-manager repos --enable=rhel-9-for-x86_64-appstream-rpms\nsudo dnf update -y \nsudo reboot\n</code></pre>"},{"location":"install/install/#download-required-tools","title":"Download Required Tools","text":"<p>Install the required tools.   </p> <pre><code>OCP_VERSION=4.19\nwget \"https://mirror.openshift.com/pub/openshift-v4/x86_64/clients/ocp/stable-${OCP_VERSION}/openshift-install-linux.tar.gz\" -P /tmp\nsudo tar -xvzf /tmp/openshift-install-linux.tar.gz -C /usr/local/bin\nwget \"https://mirror.openshift.com/pub/openshift-v4/clients/ocp/stable-${OCP_VERSION}/openshift-client-linux.tar.gz\" -P /tmp\nsudo tar -xvzf /tmp/openshift-client-linux.tar.gz -C /usr/local/bin\nrm /tmp/openshift-install-linux.tar.gz /tmp/openshift-client-linux.tar.gz -y\nsudo dnf install nmstate git podman \n</code></pre> <p>Check for the availability of the required tools.    </p> <pre><code>openshift-install version\noc version\nnmstatectl -V\ngit -v\npodman --version\n</code></pre>"},{"location":"install/install/#get-the-pull-secret","title":"Get the Pull Secret","text":"<p>Pull Secret is available at https://console.redhat.com/openshift/install/pull-secret Download to ~/.pull-secret</p>"},{"location":"install/install/#create-ssh-key","title":"Create SSH Key","text":"<pre><code>ssh-keygen -t ed25519 -f ~/.ssh/ocp_ed25519\n</code></pre>"},{"location":"install/install/#open-port-8080-for-iso-http","title":"Open Port 8080 for iso-http","text":"<pre><code>sudo firewall-cmd --permanent --add-port=8080/tcp\nsudo firewall-cmd --reload\n</code></pre>"},{"location":"install/install/#tools-for-environment-validation","title":"Tools for Environment Validation","text":"<pre><code>ping registry.redhat.io        # ICMP doesn\u2019t always work, but try\ncurl -vk https://registry.redhat.io/v2/\ndig registry.redhat.io +short\nnslookup registry.redhat.io\npodman login registry.redhat.io\n</code></pre>"},{"location":"install/install/#create-the-configurations","title":"Create the Configurations","text":"<p>The typical agent based install requires two configuration files. </p> <ul> <li><code>install-config.yaml</code></li> <li><code>agent-config.yaml</code></li> </ul> <p>The <code>install-config.yaml</code> contains all the cluster level information while the <code>agent-config.yaml</code> provides host level configurations. </p> <p>You can accomplish all of this with bastion tools like <code>vi</code>. It is highly recommended to use tools like Visual Studio Code (vscode) to help with yaml formatting. </p>"},{"location":"install/install/#create-the-working-directory","title":"Create the Working Directory","text":"<p>You are going to want to create a working directory and initialize it into a git repository. </p> <pre><code>mkdir -p ocp &amp;&amp; cd ocp\ngit init \necho \"install/\" &gt; .gitignore\necho \"# POC install notes\" &gt; notes.md\ntouch install-config.yaml\ntouch agent-config.yaml\ngit add -A\ngit commit -m \"repo initialized\"\n</code></pre>"},{"location":"install/install/#create-the-install-config","title":"Create the Install Config","text":"<p>Here is an example of the <code>install-config.yaml</code> contents.</p> <pre><code>apiVersion: v1\nbaseDomain: ocp.basedomain.com\nmetadata:\n  name: poc\ncontrolPlane:\n  name: master\n  architecture: amd64\n  hyperthreading: Enabled\n  replicas: 3\ncompute:\n- name: worker\n  architecture: amd64\n  hyperthreading: Enabled\n  replicas: 3\nnetworking:\n  clusterNetwork:\n  - cidr: 10.128.0.0/14\n    hostPrefix: 23\n  machineNetwork:\n  - cidr: 10.1.0.0/24\n  networkType: OVNKubernetes\n  serviceNetwork:\n  - 172.30.0.0/16\nplatform:\n  baremetal:\n    apiVIP: 10.1.0.9\n    ingressVIP: 10.1.0.10\n    additionalNTPServers: \n      - 0.us.pool.ntp.org\n      - 1.us.pool.ntp.org\npullSecret: 'value from ~/.pull_secret'\nsshKey: 'value from ~/.ssh/ocp_ed25519.pub'\n</code></pre> <p>If your environment requires a proxy to access the internet then copy, modify and append to the end of the <code>install-config.yaml</code></p> <pre><code>proxy:\n  httpProxy: http://user:password@proxy.example.com:3128\n  httpsProxy: http://user:password@proxy.example.com:3128\n  noProxy: poc.ocp.basedomain.com,10.1.0.0/24,10.128.0.0/14,172.30.0.0/16,api.poc.ocp.basedomain.com,api-int.poc.ocp.basedomain.com,*.apps.poc.ocp.basedomain.com\n</code></pre>"},{"location":"install/install/#create-the-agent-config","title":"Create the Agent Config","text":"<p>The agent-config.yaml is basically a list of hosts' information. You'll create a list item for each of the hosts. Here's an example of a host with two ethernet connections bonded together in an LACP bond. </p> <pre><code>apiVersion: v1alpha1\nkind: AgentConfig\nmetadata:\n  name: poc\nrendezvousIP: 10.1.0.11                     # IP of the first control-plane1 host\nadditionalNtpSources:\n  - 0.us.pool.ntp.org\n  - 1.us.pool.ntp.org\nhosts:\n  - hostname: openshift-control-plane-1     # hostname\n    role: master                            # master/worker\n    rootDeviceHints:\n      deviceName: \"/dev/sda\"                # Disk Hint\n    interfaces:\n      - name: eno1                          # Interface Name 1\n        macAddress: A1:B2:3C:4D:1E:11       # Mac Address 1\n      - name: eno2                          # Interface Name 2\n        macAddress: A1:B2:3C:4D:2E:11       # Mac Address 2\n    networkConfig:\n      interfaces:\n        - name: eno1                        # Interface Name 1\n          type: ethernet\n          state: up\n          mac-address: A1:B2:3C:4D:1E:11    # Mac Address 1\n          ipv4:\n            enabled: false\n          ipv6:\n            enabled: false\n        - name: eno2                        # Interface Name 2\n          type: ethernet\n          state: up\n          mac-address: A1:B2:3C:4D:2E:11    # Mac Address 2\n          ipv4:\n            enabled: false\n          ipv6:\n            enabled: false\n        - name: bond0\n          type: bond\n          state: up\n          ipv4:\n            enabled: true\n            address:\n              - ip: 10.1.0.11               # Host IP inside the machine subnet\n                prefix-length: 24\n            dhcp: false\n          link-aggregation:\n            mode: 802.3ad                   # LACP\n            port:\n              - eno1\n              - eno2\n            options:\n              miimon: \"100\"\n              lacp_rate: fast\n          ipv6:\n            enabled: false\n      dns-resolver:\n        config:\n          server:\n            - dns1.basedomain.com           # DNS 1\n            - dns2.basedomain.com           # DNS 2\n      routes:\n        config:\n          - destination: 0.0.0.0/0\n            next-hop-address: 10.1.0.1\n            next-hop-interface: bond0\n            table-id: 254\n</code></pre> <p>Notice the inconsistent labels and spellings.   * <code>macAddress</code> in the interfaces stanza, but <code>mac-address</code> in the networkConfig stanza.     * <code>additionalNtpSources</code> is used here, but it's <code>additionalNTPServers</code> in the <code>install-config.yaml</code></p> <p>Docs: Sample Config Bonds Vlans</p>"},{"location":"install/install/#generate-the-iso","title":"Generate the ISO","text":"<p>From the <code>~/ocp</code> directory, you can now create the agent iso. We create an install directory and copy the yaml files into the directory because the image creation process consumes and destroys the configuration files. We want to keep a copy in case the process needs to be repeated.</p> <p>Here's an example script. I would recommend creating a small file named <code>create-iso.sh</code> in the <code>ocp</code> working directory with the contents. Don't forget to <code>chmod +x create-iso.sh</code></p> <pre><code>#!/bin/bash\nrm -rf install\nmkdir install\ncp -r install-config.yaml agent-config.yaml install\nopenshift-install agent create image --dir=install\n</code></pre> <p>You can always add <code>--log-level=debug</code> to any <code>openshift-install</code> commands for more output. </p> <p>This is going to generate an <code>agent.x86_64.iso</code> file in the install directory which should be located at <code>~/ocp/install</code>. Remember, everything in the <code>install</code> directory has been excluded from the git repository. </p>"},{"location":"install/install/#boot-the-machines","title":"Boot the Machines","text":"<p>The best way to to provide the ISO as an HTTP source if you are using a BMC like iLo or iDRAC. Not doing so could result in issues during the install due to network contention using the web interfaces of those tools. </p> <p>Luckily, on the bastion host, we already have podman installed so we can just use that to serve. </p> <pre><code>podman run -d --name iso-http \\\n  -p 8080:8080 \\\n  -v ~/ocp/install/agent.x86_64.iso:/var/www/html/agent.x86_64.iso:Z \\\n  registry.redhat.io/rhel9/httpd-24:9.6\n</code></pre> <p>Test retrieving the iso using the following command. </p> <pre><code>wget http://bastionhostname:8080/agent.x86_64.iso\n</code></pre> <p>Don't forget to open the host firewall on the bastion.   </p> <p>If for some reason this solution will not work, you can install httpd and serve from there. </p> <p>You can also copy it somewhere else from where the BMC has better access. Here's an example using scp.</p> <pre><code>scp user@192.168.122.187:~/ocp/install/agent.x86_64.iso ~/iso/\n</code></pre>"},{"location":"install/install/#install-the-cluster","title":"Install the Cluster","text":"<p>When all the hosts are booted, wait for the bootstrap to complete. </p> <pre><code>openshift-install agent wait-for bootstrap-complete --dir=install\n</code></pre> <p>When the bootstrap is complete, wait for the install to complete. </p> <pre><code>openshift-install agent wait-for install-complete --dir=install\n</code></pre> <p>At the end of the process, you will be presented with the URL for the cluster endpoint, along with the kubeadmin credentials. They are also available in the install folder at <code>~/ocp/install</code> as files <code>kubeadmin-password</code> and <code>kubeconfig</code></p> <p>Open the URL presented and log in. Wait until all checks are green before proceeding. </p>"},{"location":"install/install/#validate-the-install","title":"Validate the Install","text":"<p>Login to the Cluster</p> <pre><code>oc login --server=https://api.cluster.basedomain.com:6443 -u kubeadmin -p &lt;password&gt;\n</code></pre> <p>Test Connectivity</p> <pre><code>oc debug node/&lt;worker-node-name&gt; -- chroot /host \\\n  podman pull registry.redhat.io/ubi9/ubi:latest\n</code></pre> <p>Cleanup the leftover install and configuration pods</p> <pre><code>oc delete pods --all-namespaces --field-selector=status.phase=Succeeded\noc delete pods --all-namespaces --field-selector=status.phase=Failed\n</code></pre>"},{"location":"install/install/#install-debugging","title":"Install Debugging","text":"<p>Modify the Boot Parameters (GRUB/Boot Menu)</p> <p>This is often the most reliable way to get a shell or verbose output when direct TTY switching fails. You'll need to intercept the boot process.</p> <ul> <li>Reboot the machine with the ISO.</li> <li>At the GRUB (bootloader) menu: As soon as you see the initial boot menu, press an arrow key (up/down) to stop the automatic countdown.</li> <li>Edit the boot entry:<ul> <li>Select the installer's default boot entry (usually the first one).</li> <li>Press the e key to edit the boot parameters.</li> <li>Locate the linux or linuxefi line: This line contains the kernel arguments. Add debug/shell parameters to force an emergency shell. Go to the end of the linux or linuxefi line and add the param (see choices below). This will drop you into an initramfs shell before the root filesystem is mounted. It's a very minimal environment but allows ip a show, dmesg, and looking at files in the initramfs.</li> <li>Press Ctrl+X or F10 to boot with the modified parameters.</li> </ul> </li> </ul> <p>The three main alternatives are:</p> <p><code>rd.break</code></p> <p>How it works: This parameter tells dracut (the initramfs environment) to interrupt the normal boot process before the real root filesystem is mounted and before systemd takes over. You are dropped into an initramfs shell where you can manually mount <code>/sysroot</code> and chroot into it to make changes. Since this is very early in the boot sequence, the environment is limited, but it lets you work on the system before anything else initializes.</p> <p>When to use it: This is the method of choice for situations where you need to fix problems directly on the root filesystem, especially when you can\u2019t log in normally\u2014such as resetting a forgotten root password or repairing critical files before systemd runs. It\u2019s lower-level than <code>systemd.unit=emergency.target</code> and gives you control before the system even mounts root.</p> <p><code>systemd.unit=emergency.target</code></p> <p>How it works: This parameter tells the systemd process to boot directly into a minimal shell. It will try to mount the root filesystem as read-only and start only the most essential services required for an emergency shell. This is often the preferred method for general system troubleshooting.</p> <p>When to use it: This is a good choice for fixing issues that aren't related to the initial filesystem or the boot process itself, such as a corrupt /etc/fstab or a misconfigured service. It gives you a more complete environment than rd.break but still keeps things simple.</p> <p><code>init=/bin/bash</code></p> <p>How it works: This is the most direct and basic method. It tells the kernel to bypass all the normal boot processes and execute /bin/bash directly as the first process (PID 1). This gives you a shell with no services, no network, and the root filesystem mounted as read-only.</p> <p>When to use it: Use this as a last resort when other methods fail. It's the most primitive and powerful method, as it gives you control before any other processes or services start. It's ideal for a corrupted boot process or severe filesystem issues where rd.break or emergency.target might fail to load. It also requires you to manually remount the root filesystem as read-write, just like you were doing with rd.break.</p>"},{"location":"install/rhel_install/","title":"Red Hat Enterprise Linux Install","text":"<p>Back</p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p>"},{"location":"networking/networking/","title":"Networking","text":""},{"location":"networking/networking/#overview","title":"Overview","text":"<p>From a linux configuration perspective, remember that: </p> <ul> <li>There are physical connections (<code>type: ethernet</code>)</li> <li>Those can be bonded (bond)<ul> <li>Bonds have a mode of <code>802.3ad</code> (LACP), <code>balance-xor</code>, or <code>active-backup</code>. </li> </ul> </li> <li>Vlans can have a <code>base-iface</code> of an existing <code>ethernet</code> or <code>bond</code><ul> <li>You can have multiple vlans from a trunked <code>base-iface</code></li> </ul> </li> </ul>"},{"location":"networking/networking/#typical-openshift-production-setup","title":"Typical OpenShift Production Setup","text":"<ul> <li>7 NICs</li> <li>3 Bonds - LACP<ul> <li>bond0 - management bond for cluster traffic</li> <li>bond1 - data bond for pod network</li> <li>bond2 - storage network</li> </ul> </li> <li>1 BMC interface</li> </ul>"},{"location":"networking/networking/#the-full-stack-for-underlay-networking","title":"The Full Stack for Underlay Networking","text":"<p>NNCP = NodeNetworkConfigurationPolicy OVN-K = Open Virtual Networking - Kubernetes (OpenShift's CNI)</p> Concept Where Managed By Switch Physical Switch Ethernet Linux NNCP Bond Linux NNCP OVS Bridge Linux NNCP OVN Bridge Mapping OVN-K NNCP Localnet OVN-K NNCP Cluster User Defined Network (CUDN) OVN-K - Network Attachment Definition OVN-K CUDN Virtual Ethernet Pair Kubernetes CNI"},{"location":"networking/networking/#descriptions","title":"Descriptions","text":""},{"location":"networking/networking/#switch-physical-layer-2-device","title":"Switch (Physical Layer 2 device)","text":"<p>What it is: The physical top-of-rack or in-rack switch (Arista, MikroTik, Juniper, etc.).   Role in OpenShift: Provides the L2 domain where your cluster nodes attach. VLAN tags, trunks, and port-channels live here.  Implementation: It\u2019s actual silicon forwarding Ethernet frames. OVN doesn\u2019t know or care about it directly, but VLANs and MTU mismatches will break your cluster if misaligned.   </p>"},{"location":"networking/networking/#ethernet-nic-inside-the-node","title":"Ethernet (NIC inside the node)","text":"<p>What it is: The real network interface card (NIC) in the node. Could be Intel, Mellanox, Solarflare, etc.  Implementation: In linux, shows up as eno1, ens3f0, etc. Uses a kernel driver (ixgbe, mlx5_core, sfc).  In OpenShift: The base network device that NodeNetworkConfigurationPolicy (NNCP) manipulates.  </p> <pre><code>- name: &lt;devicename&gt;\n  type: ethernet\n  state: up\n  mac-address: &lt;macaddress&gt;\n  ipv4:\n    enabled: false\n  ipv6:\n    enabled: false\n</code></pre>"},{"location":"networking/networking/#bond-linux-bonding-or-lacp","title":"Bond (Linux bonding or LACP)","text":"<p>What it is: A logical link aggregation across multiple physical NICs. Implementation: Linux kernel bonding driver (bonding.ko). Supports modes active-backup, 802.3ad (LACP), balance-xor. In OpenShift: Configured by the SR-IOV or NMState operators. Exposed in NNCP YAML (type: bond, link-aggregation:).</p> <pre><code>- name: bond&lt;i&gt;\n  type: bond\n  state: up\n  link-aggregation:\n    mode: 802.3ad\n    port:\n      - &lt;devicename&gt;\n      - &lt;devicename2&gt;\n    options:\n      miimon: \"100\"\n      lacp_rate: fast\n  ipv4:\n    enabled: false\n  ipv6:\n    enabled: false\n</code></pre>"},{"location":"networking/networking/#vlan","title":"VLAN","text":"<p>What it is: A logical sub-interface identified by a VLAN ID, created on top of a NIC or bond. Role in OpenShift: Segments traffic into isolated broadcast domains, allowing pods or nodes to connect to different L2 networks over the same physical link. Implementation: Linux kernel 8021q module creates sub-interfaces (e.g., bond0.31 for VLAN 31). Configured via NNCP or NetworkManager. VLAN tags are inserted/removed by the kernel before frames leave/arrive on the NIC.  </p> <pre><code>- name: bond&lt;i&gt;.&lt;vlanid&gt;\n  type: vlan\n  state: up\n  vlan:\n    base-iface: bond&lt;i&gt;\n    id: &lt;vlanid&gt;\n  ipv4:\n    enabled: true\n    address:\n      - ip: &lt;ipaddress&gt;               \n        prefix-length: &lt;prefixlength&gt; # 24\n    dhcp: false\n  ipv6:\n    enabled: false\n</code></pre>"},{"location":"networking/networking/#ovs-bridge-open-vswitch-kernel-module-daemon","title":"OVS Bridge (Open vSwitch kernel module + daemon)","text":"<p>What it is: A software switch running in each node. Implementation: Kernel datapath + ovs-vswitchd in userspace. Can do VLAN tagging, port mirroring, QoS. In OpenShift: OVN-Kubernetes uses an OVS bridge (br-int) for pod connectivity. If you\u2019re doing localnet or external connectivity, OVS bridges handle the attachment to physical NICs or bonds.  </p> <pre><code>- name: ovs-bridge-trunk\n  type: ovs-bridge\n  state: up\n  bridge:\n    port:\n      - name: bond1\n    allow-extra-patch-ports: true\n    options:\n      stp: false # DO NOT ENABLE STP unless you know what you are doing!\n</code></pre>"},{"location":"networking/networking/#ovn-bridge-mapping","title":"OVN Bridge Mapping","text":"<p>What it is: A mapping between a logical OVN bridge port and a real OVS bridge/port on the node. Implementation: Managed by the ovn-kubernetes daemonset (ovn-controller). Stored in OVSDB. Example: bridge-mappings=physnet1:br-ex tells OVN that traffic for physnet1 is reachable via the host\u2019s br-ex bridge. </p> <pre><code>desiredState:\n  ovn:\n    bridge-mappings:\n      - localnet: localnet-bridge-trunk\n        bridge: ovs-bridge-trunk\n        state: present\n</code></pre>"},{"location":"networking/networking/#localnet-ovn-construct","title":"Localnet (OVN construct)","text":"<p>What it is: A special OVN logical switch port type that maps a logical network directly to a physical network. Implementation: In OVN NB DB as type=localnet. Bypasses overlay tunnels (Geneve) and uses VLAN-backed segments. In OpenShift: Used for Cluster User Defined Networks (CUDN) with type: Localnet so pods connect to a real VLAN instead of overlay.  </p>"},{"location":"networking/networking/#cluster-user-defined-network-cudn","title":"Cluster User Defined Network (CUDN)","text":"<p>What it is: An OpenShift CRD (ClusterNetwork) that defines extra networks beyond the default pod network. Implementation: Managed by the Network Operator \u2192 writes into OVN NB DB. Can be Geneve overlay or Localnet VLAN-backed. In practice: This is how you say \u201cI want a storage network on VLAN 31, isolated from the default pod net.\u201d </p> <pre><code>apiVersion: k8s.ovn.org/v1\nkind: ClusterUserDefinedNetwork\nmetadata:\n name: vlan&lt;vlanid&gt;\nspec:\n  namespaceSelector:\n    matchExpressions:\n      - key: kubernetes.io/metadata.name\n        operator: In\n        values: [\"&lt;namespace&gt;\", \"&lt;namespace2&gt;\"] \n  network:\n    topology: Localnet\n    localnet:\n      role: Secondary\n      physicalNetworkName: localnet-bridge-trunk\n      vlan:\n        mode: Access\n        access:\n          id: &lt;vlanid&gt;\n      ipam:\n        mode: Disabled\n</code></pre>"},{"location":"networking/networking/#network-attachment-definitions-nads","title":"Network Attachment Definitions (NADs)","text":"<p>What it is: A Kubernetes CRD (network-attachment-definitions.k8s.cni.cncf.io) provided by Multus. Implementation: NAD describes which CNI plugin (OVN, SR-IOV, macvlan, whereabouts) to call when attaching extra networks to a pod.  In OpenShift: Each NAD corresponds to a net-attach-def object. Pods reference them via k8s.v1.cni.cncf.io/networks.  </p> <pre><code>apiVersion: k8s.cni.cncf.io/v1\nkind: NetworkAttachmentDefinition\nmetadata:\n  name: vlan&lt;vlanid&gt;\n  namespace: &lt;namespace&gt;\n  ownerReferences:\n    - apiVersion: k8s.ovn.org/v1\n      blockOwnerDeletion: true\n      controller: true\n      kind: ClusterUserDefinedNetwork\n      name: vlan&lt;vlanid&gt;\nspec:\n  config: '\n    {\n      \"cniVersion\":\"1.0.0\",\n      \"mtu\":1500,\n      \"name\":\"cluster_udn_vlan&lt;vlanid&gt;\",\n      \"netAttachDefName\":\"&lt;namespace&gt;/vlan&lt;vlanid&gt;\",\n      \"physicalNetworkName\":\"localnet-bridge-trunk\",\n      \"role\":\"secondary\",\n      \"topology\":\"localnet\",\n      \"type\":\"ovn-k8s-cni-overlay\",\n      \"vlanID\":&lt;vlanid&gt;\n    }'\n</code></pre>"},{"location":"networking/networking/#virtual-ethernet-pair-veth","title":"Virtual Ethernet Pair (veth)","text":"<p>What it is: The plumbing pipe. Implementation: Linux kernel primitive (ip link add veth0 type veth peer name veth1). In OpenShift: When a pod is created, Multus/OVN creates a veth pair. One end goes into the pod\u2019s netns (eth0). The other end plugs into br-int (OVS) or another bridge. OVN programs flows to steer traffic between veth, tunnels, and physical NICs.  </p>"},{"location":"networking/networking/#pod-kubernetes-abstraction","title":"Pod (Kubernetes abstraction)","text":"<p>What it is: A Linux process in a container namespace. Networking view: Sees its veth eth0, a private IP, and the routes injected by the CNI plugin. In OpenShift: The pod doesn\u2019t know about OVN, Multus, or OVS\u2014it just thinks it has a NIC. Behind the curtain, OVN is doing NAT, overlay encapsulation (Geneve), or VLAN bridging depending on your network config.  </p>"},{"location":"networking/networking/#nodenetworkconfigurationpolicy-examples","title":"NodeNetworkConfigurationPolicy Examples","text":"<p>This is just for setting up your bonds. </p> 2-eth-bond-lacp-vlan <pre><code>apiVersion: nmstate.io/v1\nkind: NodeNetworkConfigurationPolicy\nmetadata:\n  name: 2-eth-bond-lacp-vlan\nspec:\n  nodeSelector:\n    kubernetes.io/hostname: &lt;hostname&gt;\n  desiredState:\n    interfaces:\n      - name: &lt;devicename&gt;\n        type: ethernet\n        state: up\n        mac-address: &lt;macaddress&gt;\n        ipv4:\n          enabled: false\n        ipv6:\n          enabled: false\n      - name: &lt;devicename2&gt;\n        type: ethernet\n        state: up\n        mac-address: &lt;macaddress2&gt;\n        ipv4:\n          enabled: false\n        ipv6:\n          enabled: false\n      - name: bond1\n        type: bond\n        state: up\n        link-aggregation:\n          mode: 802.3ad\n          port:\n            - &lt;devicename&gt;\n            - &lt;devicename2&gt;\n          options:\n            miimon: \"100\"\n            lacp_rate: fast\n        ipv4:\n          enabled: false\n        ipv6:\n          enabled: false\n      - name: bond1.&lt;vlanid&gt;\n        type: vlan\n        state: up\n        vlan:\n          base-iface: bond1\n          id: &lt;vlanid&gt;\n        ipv4:\n          enabled: true\n          address:\n            - ip: &lt;ipaddress&gt;               \n              prefix-length: &lt;prefixlength&gt; # 24\n          dhcp: false\n        ipv6:\n          enabled: false\n</code></pre> <p>Another example for setting up your bonds with <code>active-backup</code>. </p> 2-eth-bond-active-backup-vlan <pre><code>apiVersion: nmstate.io/v1\nkind: NodeNetworkConfigurationPolicy\nmetadata:\n  name: 2-eth-bond-active-backup-vlan\nspec:\n  nodeSelector:\n    kubernetes.io/hostname: &lt;hostname&gt;\n  desiredState:\n    interfaces:\n      - name: &lt;devicename&gt;\n        type: ethernet\n        state: up\n        mac-address: &lt;macaddress&gt;\n        ipv4:\n          enabled: false\n        ipv6:\n          enabled: false\n      - name: &lt;devicename2&gt;\n        type: ethernet\n        state: up\n        mac-address: &lt;macaddress2&gt;\n        ipv4:\n          enabled: false\n        ipv6:\n          enabled: false\n      - name: bond1\n        type: bond\n        state: up\n        link-aggregation:\n          mode: active-backup\n          port:\n            - &lt;devicename&gt;\n            - &lt;devicename2&gt;\n          options:\n            miimon: '100'\n            primary: &lt;devicename&gt;\n        ipv4:\n          enabled: false\n        ipv6:\n          enabled: false\n      - name: bond1.&lt;vlanid&gt;\n        type: vlan\n        state: up\n        vlan:\n          base-iface: bond1\n          id: &lt;vlanid&gt;\n        ipv4:\n          enabled: true\n          address:\n            - ip: &lt;ipaddress&gt;               \n              prefix-length: &lt;prefixlength&gt; # 24\n          dhcp: false\n        ipv6:\n          enabled: false\n</code></pre> <p>This is for when you want to create a trunk all the way to the OpenShift SDN.</p> ovs-bridge-trunk-nncp <p>This example assumes you have an existing `bond1` on each of your worker nodes. One of the good things about using bonded connections is that if your device names are not consistent across your hardware, you have the ability to obfuscate the differences and name the bonds all the same, thus your additional configurations can be applied to larger groupings of hardware. </p> <pre><code>apiVersion: nmstate.io/v1\nkind: NodeNetworkConfigurationPolicy\nmetadata:\n  name: ovs-bridge-trunk-nncp\nspec:\n  nodeSelector:\n    node-role.kubernetes.io/worker: \"\"\n  desiredState:\n    interfaces:\n      - name: ovs-bridge-trunk\n        type: ovs-bridge\n        state: up\n        bridge:\n          port:\n            - name: bond1\n          allow-extra-patch-ports: true\n          options:\n            stp: false # DO NOT ENABLE STP unless you know what you are doing!\n    ovn:\n      bridge-mappings:\n        - localnet: localnet-bridge-trunk\n          bridge: ovs-bridge-trunk\n          state: present\n</code></pre>"},{"location":"networking/networking/#clusteruserdefinednetwork","title":"ClusterUserDefinedNetwork","text":"<p>Once you have your NNCPs in place and your underlays are created, now you need to take it that last mile to the namespaces. </p> cudn-with-ipam <p>This example assumes you have an existing ovn bridge mapping for a localnet connection called localnet-bridge-trunk.</p> <pre><code>apiVersion: k8s.ovn.org/v1\nkind: ClusterUserDefinedNetwork\nmetadata:\n name: cudn-with-ipam\nspec:\n  namespaceSelector:\n    matchExpressions:\n      - key: kubernetes.io/metadata.name\n        operator: In\n        values: [\"&lt;namespace&gt;\", \"&lt;namespace2&gt;\"] \n  network:\n    topology: Localnet\n    localnet:\n      role: Secondary\n      physicalNetworkName: localnet-bridge-trunk\n      vlan:\n        mode: Access\n        access:\n          id: 4\n      subnets:\n        - \"10.4.0.0/24\"\n      excludeSubnets:\n        - \"10.4.0.0/31\"       # excludes 10.4.0.0 \u2013 10.4.0.1\n        - \"10.4.0.255/32\"     # excludes 10.4.0.255\n      ipam:\n        mode: Enabled         # DEFAULT!\n        lifecycle: Persistent\n</code></pre> cudn-no-ipam <p>This example assumes you have an existing ovn bridge mapping for a localnet connection called localnet-bridge-trunk.</p> <pre><code>apiVersion: k8s.ovn.org/v1\nkind: ClusterUserDefinedNetwork\nmetadata:\n name: cudn-no-ipam\nspec:\n  namespaceSelector:\n    matchExpressions:\n      - key: kubernetes.io/metadata.name\n        operator: In\n        values: [\"&lt;namespace&gt;\", \"&lt;namespace2&gt;\"] \n  network:\n    topology: Localnet\n    localnet:\n      role: Secondary\n      physicalNetworkName: localnet-bridge-trunk\n      vlan:\n        mode: Access\n        access:\n          id: 4\n      ipam:\n        mode: Disabled\n</code></pre>"},{"location":"notes/create-machine-config/","title":"Creating a Machine Configuration File","text":"<p>Back</p> <p>How to base64 encode a string</p> <pre><code>echo -n \"mysecretvalue\" | base64\n</code></pre> <p>To decode</p> <pre><code>echo -n \"bXlzZWNyZXR2YWx1ZQ==\" | base64 -d\n</code></pre> <p>How to encode a file</p> <pre><code>#encode\nbase64 -w0 /path/to/file\n#decode\nbase64 /path/to/file &gt; file.b64\n#decode\nbase64 -d file.b64 &gt; file.decoded\n</code></pre> <p>Then you take the base64 encoded value and paste it into the file after the <code>base64,</code></p> <pre><code>apiVersion: machineconfiguration.openshift.io/v1\nkind: MachineConfig\nmetadata:\n  name: 99-worker-example\n  labels:\n    machineconfiguration.openshift.io/role: worker\nspec:\n  config:\n    ignition:\n      version: 3.4.0\n    storage:\n      files:\n      - path: /etc/file.conf\n        mode: 0644\n        overwrite: true\n        contents:\n          source: data:text/plain;charset=utf-8;base64,bXlzZWNyZXR2YWx1ZQ==\n</code></pre>"},{"location":"notes/notes/","title":"Notes","text":"<p>Creating a Machine Configuration File</p>"},{"location":"postinstall/add-worker-node/","title":"Adding a Worker Node to an Existing Cluster","text":"<p>Back to Main</p>"},{"location":"postinstall/add-worker-node/#adding-a-worker-node-to-an-existing-bare-metal-on-premise-openshift-cluster","title":"Adding a Worker Node to an existing bare-metal on-premise OpenShift cluster","text":"<p>This guide describes the steps to add a new worker node to an existing OpenShift cluster. This example assumes you have a bare-metal on-premise OpenShift cluster and you want to add an additional worker node.</p>"},{"location":"postinstall/add-worker-node/#create-the-yaml-configuration-file","title":"Create the YAML configuration file","text":"<p>Create a file named <code>nodes-config.yaml</code> with the following content. Modify the values to match your environment. This shuld look very familiar as it is similar to the install-config.yaml file used during installation. If you are readding a node to an existing cluster, you can copy the install-config.yaml file and modify it as needed.</p> <pre><code>hosts:\n- hostname: extra-worker-1\n  rootDeviceHints:\n   deviceName: /dev/sda\n  interfaces:\n   - macAddress: 00:00:00:00:00:00\n     name: eth0\n  networkConfig:\n   interfaces:\n     - name: eth0\n       type: ethernet\n       state: up\n       mac-address: 00:00:00:00:00:00\n       ipv4:\n         enabled: true\n         address:\n           - ip: 192.168.122.2\n             prefix-length: 23\n         dhcp: false\n</code></pre> <p>YAML file parameters</p> nodes-config.yaml with bond0 <p>This example includes two nics and a bond.</p> <pre><code>hosts: \n  - hostname: openshift-worker-x            # hostname\n    role: worker                            # master/worker\n    rootDeviceHints:\n      deviceName: \"/dev/sda\"                # Disk Hint\n    interfaces:\n      - name: eno1                          # Interface Name 1\n        macAddress: A1:B2:3C:4D:1E:11       # Mac Address 1\n      - name: eno2                          # Interface Name 2\n        macAddress: A1:B2:3C:4D:2E:11       # Mac Address 2\n    networkConfig:\n      interfaces:\n        - name: eno1                        # Interface Name 1\n          type: ethernet\n          state: up\n          mac-address: A1:B2:3C:4D:1E:11    # Mac Address 1\n          ipv4:\n            enabled: false\n          ipv6:\n            enabled: false\n        - name: eno2                        # Interface Name 2\n          type: ethernet\n          state: up\n          mac-address: A1:B2:3C:4D:2E:11    # Mac Address 2\n          ipv4:\n            enabled: false\n          ipv6:\n            enabled: false\n        - name: bond0\n          type: bond\n          state: up\n          ipv4:\n            enabled: true\n            address:\n              - ip: 10.1.0.11               # Host IP inside the machine subnet\n                prefix-length: 24\n            dhcp: false\n          link-aggregation:\n            mode: 802.3ad                   # LACP\n            port:\n              - eno1\n              - eno2\n            options:\n              miimon: \"100\"\n              lacp_rate: fast\n          ipv6:\n            enabled: false\n      dns-resolver:\n        config:\n          server:\n            - dns1.basedomain.com           # DNS 1\n            - dns2.basedomain.com           # DNS 2\n      routes:\n        config:\n          - destination: 0.0.0.0/0\n            next-hop-address: 10.1.0.1\n            next-hop-interface: bond0\n            table-id: 254\n</code></pre>"},{"location":"postinstall/add-worker-node/#create-the-iso-image","title":"Create the ISO image","text":"<p>Run the following command to create the ISO image. Modify the parameters to match your environment.</p> <pre><code>oc adm node-image create nodes-config.yaml --dir=add --registry-config=/path/to/pull-secret.txt \n</code></pre> <p>Command flag options</p> <p>Verify that a new <code>node.&lt;arch&gt;.iso</code> file is present in the asset directory. The asset directory is your current directory, unless you specified a different one when creating the ISO image.  </p> <p>Boot the node with the generated ISO image.  </p>"},{"location":"postinstall/add-worker-node/#monitor-the-progress","title":"Monitor the progress","text":"<pre><code>oc adm node-image monitor --ip-addresses &lt;ip_addresses&gt;\n</code></pre> <p><code>&lt;ip_addresses&gt;</code> is a comma-separated list of IP addresses assigned to the new nodes.</p>"},{"location":"postinstall/add-worker-node/#approve-any-csrs-that-are-pending","title":"Approve any CSRs that are pending","text":"<pre><code>oc get csr\noc get csr | awk '{print $1}' | grep -v NAME | xargs oc adm certificate approve\n</code></pre>"},{"location":"postinstall/add-worker-node/#documentation-links","title":"Documentation Links","text":"<ul> <li>Adding worker node to an on-premise cluster</li> </ul>"},{"location":"postinstall/configure-registry/","title":"Configuring the OpenShift Registry","text":"<p>Back</p> <p>The OpenShift Container Platform internal image registry is deployed by default as part of the installation. By default, the registry uses ephemeral storage, which means that images stored in the registry are lost if the registry pod is rescheduled. To provide persistent storage for the registry, you can configure it to use a Persistent Volume Claim (PVC).</p> <pre><code>cat &lt;&lt;EOF | oc apply -f -\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n name: registry-storage-pvc\n namespace: openshift-image-registry\nspec:\n accessModes:\n - ReadWriteMany\n resources:\n   requests:\n     storage: 100Gi\n storageClassName: ocs-storagecluster-cephfs\nEOF\n</code></pre> <p>Once the PVC is created, you can patch the image registry configuration to use the PVC for storage.  </p> <pre><code>oc patch config.image/cluster -p '{\"spec\":{\"managementState\":\"Managed\",\"replicas\":2,\"storage\":{\"managementState\":\"Unmanaged\",\"pvc\":{\"claim\":\"registry-storage-pvc\"}}}}' --type=merge\n</code></pre>"},{"location":"postinstall/configure-registry/#documentation","title":"Documentation","text":"<p>Configuring the Registry Configuring with ODF </p>"},{"location":"postinstall/postinstall/","title":"Post Install","text":""},{"location":"postinstall/postinstall/#additional-networking","title":"Additional Networking","text":"<p>Add additional networking if required. See Networking documentation.</p>"},{"location":"postinstall/postinstall/#storage","title":"Storage","text":"<p>Install storage...</p>"},{"location":"postinstall/postinstall/#after-storage","title":"After Storage","text":"<ul> <li>Configure Registry</li> <li>Install OpenShift Virtualization </li> <li>Kubernetes NMState Operator</li> <li>Node Feature Discovery</li> <li>Workload Availability for Red Hat OpenShift<ul> <li>Node Health Check Operator</li> <li>Self Node Remediation Operator</li> <li>Kube Descheduler Operator</li> </ul> </li> <li>Cert-manager Operator for Red Hat OpenShift  </li> </ul>"},{"location":"postinstall/postinstall/#optional","title":"Optional","text":"<ul> <li>Add Worker Node </li> <li>Rotate SSH Keys</li> </ul>"},{"location":"postinstall/rotate-ssh-keys/","title":"Rotate SSH Keys after OpenShift Install","text":"<p>Back to Main</p> <p>Below is the procedure to rotate the SSH keys. </p> <pre><code>ssh-keygen -t rsa -b 4096 -f ~/.ssh/new_ocp_key -C \"ocp-key-rotation\"\noc get machineconfig 99-master-ssh -o yaml &gt; 99-master-ssh.yaml\n</code></pre> <pre><code>apiVersion: machineconfiguration.openshift.io/v1\nkind: MachineConfig\nmetadata:\n  labels:\n    machineconfiguration.openshift.io/role: master\n  name: 99-master-ssh\nspec:\n  config:\n    ignition:\n      version: 3.2.0\n    passwd:\n      users:\n      - name: core\n        sshAuthorizedKeys:\n        - ssh-rsa AAAA...your_old_key_here...\n        - ssh-rsa AAAA...your_new_key_here...\n</code></pre> <pre><code>oc apply -f 99-master-ssh.yaml\n</code></pre> <p>Repeat for Worker Nodes: If you also want to rotate keys for worker nodes, repeat steps 2-4 using 99-worker-ssh instead of 99-master-ssh</p>"},{"location":"storage/storage/","title":"Storage","text":""},{"location":"virtualization/","title":"OpenShift Virtualization","text":""},{"location":"virtualization/#prerequisites","title":"Prerequisites","text":"<ul> <li>Cluster worker nodes have been installed on bare metal</li> <li>Hardware requirements have been met</li> <li>Storage has been configured for the cluster<ul> <li>A default OpenShift Virtualization or OpenShift Container Platform storage class has been created</li> <li>To mark a storage class as the default for virtualization workloads, set the annotation <code>storageclass.kubevirt.io/is-default-virt-class</code> to <code>true</code>. </li> <li>Don't forget to read about the need for RWX</li> </ul> </li> <li>NMState Operator has been installed<ul> <li>All networking setup in terms of underlay networks have been created</li> <li>Optional, but recommended, dedicated network for live migration</li> </ul> </li> </ul>"},{"location":"virtualization/#install","title":"Install","text":"<ul> <li>Install the Openshift Virtualization Operator via the web console</li> </ul>"},{"location":"virtualization/#example-cloud-init-for-static-ip","title":"Example cloud-init for static ip","text":"<pre><code>networkData: |\n  version: 2\n  ethernets:\n    eth1:\n      dhcp4: no\n      addresses:\n        - 10.37.0.50/24\n      gateway4: 10.37.0.1\n      nameservers:\n        addresses:\n          - 10.3.0.3\n          - 9.9.9.9\n      dhcp6: no\n      accept-ra: false\nuserData: |\n  #cloud-config\n  user: cloud-user\n  password: Pass123!\n  chpasswd:\n    expire: false\n</code></pre>"}]}